{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 64\n",
    "batch_size = 64\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OM_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OM_Encoder,self).__init__()\n",
    "        self.fc1 = nn.Linear(3,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(256,128)\n",
    "        self.mean_fc = nn.Linear(128,128)\n",
    "        self.logstddev_fc = nn.Linear(128,128)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.squeeze()\n",
    "        n, c, k = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        pooled = F.max_pool1d(x, k).expand(x.size())\n",
    "        x = torch.cat([x,pooled],dim=1)\n",
    "\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        n,k,c = x.size()\n",
    "\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        pooled = F.max_pool1d(x, k)\n",
    "        pooled = pooled.expand(x.size())\n",
    "\n",
    "        x = torch.cat([x,pooled],dim=1)\n",
    "\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        n,k,c = x.size()\n",
    "\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.max_pool1d(x, k)\n",
    "        \n",
    "        x= x.squeeze()\n",
    "\n",
    "        mean = self.mean_fc(x)\n",
    "        stddev = self.logstddev_fc(x)\n",
    "        \n",
    "        return mean,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block,self).__init__()\n",
    "        self.fc1 = nn.Conv2d(256,256,kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(256,256,kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.gammaLayer1 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.gammaLayer2 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer1 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer2 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        \n",
    "    def forward(self,y):\n",
    "        x = y['ex']\n",
    "        n,c,k,d = x.size()\n",
    "\n",
    "        encoding = y['enc']\n",
    "        gamma = self.gammaLayer1(encoding)\n",
    "\n",
    "        #Need to stack the beta and gamma\n",
    "        #so that we multiply all the points for one mesh\n",
    "        #by the same value\n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "        \n",
    "        beta = self.betaLayer1(encoding)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "\n",
    "        #First apply Conditional Batch Normalization\n",
    "        out = gamma*self.bn1(x) + beta\n",
    "        #Then ReLU activation function\n",
    "        out = F.relu(out)\n",
    "        #fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        #Second CBN layer\n",
    "        gamma = self.gammaLayer2(encoding)\n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "\n",
    "        beta = self.betaLayer2(encoding)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "        \n",
    "        out = gamma* self.bn2(out) + beta\n",
    "        #RELU activation\n",
    "        out = F.relu(out)\n",
    "        #2nd fully connected\n",
    "        out = self.fc2(out)\n",
    "        #Add to the input of the ResNet Block \n",
    "        out = x + out\n",
    "        \n",
    "        return {'ex':out, 'enc':encoding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OccupancyModel,self).__init__()\n",
    "        self.blocks = self.makeBlocks()\n",
    "        self.encoderModel = OM_Encoder()\n",
    "        self.gammaLayer = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.cbn = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.fc1 = nn.Conv2d(3,256,kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(256,1,kernel_size=1)\n",
    "        \n",
    "    def makeBlocks(self):\n",
    "        blocks = []\n",
    "        for _ in range(5):\n",
    "            blocks.append(Block())\n",
    "        return nn.Sequential(*blocks)\n",
    "   \n",
    "    def sampleFromZDist(self, z):\n",
    "        mean, logstddev = z\n",
    "        std = logstddev.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(logstddev,requires_grad=True)\n",
    "        return eps.mul(std).add_(mean)\n",
    "        \n",
    "    def forward(self,x, z_eval=None):\n",
    "        if self.training:\n",
    "            z_dist = self.encoderModel(x)\n",
    "            z = self.sampleFromZDist(z_dist)\n",
    "            z = z.unsqueeze(-1)\n",
    "        else:\n",
    "            z = z_eval\n",
    "            z_dist = (0,1)\n",
    "        x = self.fc1(x)\n",
    "        #5 pre-activation ResNet-blocks\n",
    "        x = self.blocks({'enc':z, 'ex':x })\n",
    "        x = x['ex']\n",
    "        n,c,k,d = x.size()\n",
    "        \n",
    "        #CBN\n",
    "        gamma = self.gammaLayer(z)\n",
    "        \n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "        \n",
    "        beta = self.betaLayer(z)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "\n",
    "        x = gamma.mul(self.cbn(x)).add_(beta)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = x.view(-1,1)\n",
    "        #x = torch.sigmoid(x)\n",
    "        return x, z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om = OccupancyModel()\n",
    "optimizer = optim.Adam(om.parameters(), lr = 0.001)\n",
    "om.train()\n",
    "om.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a category and load all of the available data:\n",
    "import random\n",
    "topdir = \"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks\"\n",
    "\n",
    "#One DataSetClass per subdirectory in a category, will return \"K\" point samples and a single image randomly\n",
    "#drawn from the 23 available\n",
    "class DataSetClass(torch.utils.data.Dataset):\n",
    "    def __init__(self, d):\n",
    "        self.dir = d\n",
    "        with numpy.load(f\"{d}/points.npz\") as data:\n",
    "            self.pts = torch.tensor(data[\"points\"], dtype=torch.float)\n",
    "            self.occupancies = torch.tensor(numpy.unpackbits(data[\"occupancies\"])[:self.pts.size()[0]], dtype=torch.float)\n",
    "        self.K = K \n",
    "        self.length = int(self.occupancies.size()[0]/self.K)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.pts[idx*self.K:(idx*self.K+self.K)], self.occupancies[idx*self.K:(idx*self.K+self.K)]\n",
    "\n",
    "       \n",
    "#catalogue all of the directories with the chosen category\n",
    "trainingDirs = []\n",
    "couchesDirectory=f\"{topdir}/data/ShapeNet/04256520\"\n",
    "#couchesDirectory=f\"{topdir}/data/ShapeNet/02828884\"\n",
    "with io.open(f\"{couchesDirectory}/train.lst\") as trainlist:\n",
    "    for traindir in trainlist.readlines():\n",
    "        trainingDirs.append(f\"{couchesDirectory}/{traindir.strip()}\")\n",
    "dataSets = []\n",
    "for tdir in trainingDirs:\n",
    "    dataSets.append(DataSetClass(tdir))\n",
    "data = torch.utils.data.ConcatDataset(dataSets)\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "#Get the validation data\n",
    "valDirs = []\n",
    "with io.open(f\"{couchesDirectory}/val.lst\") as vallist:\n",
    "    for valdir in vallist.readlines():\n",
    "        valDirs.append(f\"{couchesDirectory}/{valdir.strip()}\")\n",
    "dataSets = []\n",
    "for vdir in valDirs:\n",
    "    dataSets.append(DataSetClass(vdir))\n",
    "val_data = torch.utils.data.ConcatDataset(dataSets)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, trainloader, optimizer):\n",
    "    decoderLoss = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    #encoderLoss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        (pts, occupancies) = data\n",
    "        #Each batch size contains batch_size sets of \"K\" points\n",
    "        pts = pts.view(-1,K, 3,1).permute(0,2,1,3).cuda()\n",
    "        occupancies = occupancies.view(-1,K,1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred,z_dist = model(pts) #a probability for each point, and the dist parameters of latent distribution\n",
    "        pred = pred.permute(0,2,1,3).squeeze(-1)\n",
    "        #targetNormal = torch.stack((torch.zeros_like(z_dist[0]),torch.ones_like(z_dist[1])))\n",
    "        #encloss = encoderLoss(torch.stack(z_dist),targetNormal)\n",
    "        mu, log_var = z_dist\n",
    "        encloss = mu.pow(2).add_(log_var.exp()).mul_(-1).add_(1).add_(log_var)\n",
    "        encloss = torch.sum(encloss).mul_(-0.5)\n",
    "        #encloss = -0.5*torch.sum(1+z_dist[1] + z_dist[0].pow(2) - z_dist[1].exp())\n",
    "        #print(encloss)\n",
    "        decloss = decoderLoss(pred,occupancies)\n",
    "        \n",
    "        loss = (encloss + decloss/K)/batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "            #print(f\"Reconstruction Loss {decloss/(K*batch_size)}\")\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Saving to model3.pth\")\n",
    "            torch.save(model.state_dict(), \"unconditional_model3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(0,om,train_loader,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(1,10):\n",
    "    train(ep,om,train_loader,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OccupancyModel()\n",
    "model.load_state_dict(torch.load(\"unconditional_model3.pth\",map_location=device))\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def validation(model, val_loader):\n",
    "    model.eval()\n",
    "    decoderLoss = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, data in enumerate(val_loader):\n",
    "        (pts, occupancies) = data\n",
    "        pts = pts.view(-1,K, 3,1).permute(0,2,1,3).cuda()\n",
    "        occupancies = occupancies.view(-1,K,1).cuda()\n",
    "        z = Variable(torch.randn(pts.size()[0],128))\n",
    "        z= z.unsqueeze(-1).cuda()\n",
    "        pred,z_dist = model(pts,z) \n",
    "        pred = pred.permute(0,2,1,3).squeeze(-1)\n",
    "        pred = torch.sigmoid(pred)\n",
    "        #print(pred)\n",
    "        #print(occupancies)\n",
    "        loss = decoderLoss(pred, occupancies)\n",
    "        validation_loss += loss.item()\n",
    "        \n",
    "        threshold = 0.6\n",
    "        roundedOut = [1 if out > threshold else 0 for out in pred.view(-1)]\n",
    "        roundedOut = torch.tensor(roundedOut).cuda()\n",
    "        correctNow = roundedOut.eq(occupancies.view(-1)).sum()\n",
    "        correct += correctNow\n",
    "        validation_loss /= len(val_loader.dataset)\n",
    "        print('Validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correctNow, pts.size()[0]*K, 100. * correctNow / (pts.size()[0]*K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
