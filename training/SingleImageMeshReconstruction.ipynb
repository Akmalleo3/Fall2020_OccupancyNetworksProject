{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Encoder network for single view 3D reconstruction is a ResNet18 pretrained\n",
    "#on the ImageNet dataset with the last fully-connected layer adjusted to project\n",
    "#the features to a 256 dimensional embedding, \"c\"\n",
    "from torchvision.models.resnet import resnet18 as _resnet18\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 25\n",
    "batch_size = 25\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder,self).__init__()\n",
    "        self.encoderModel = _resnet18(pretrained=True)\n",
    "        self.fc1 = nn.Linear(1000, 256)\n",
    "        self.betafc = nn.Linear(256,256)\n",
    "        self.gammafc = nn.Linear(256,256)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoderModel(x)\n",
    "        #project to 256 dimensional embedding \n",
    "        x = self.fc1(x)\n",
    "        # Obtain Beta and gamma inputs into conditional batch normalization\n",
    "        # QUESTION Are these split or one after the other?\n",
    "        beta = self.betafc(x)\n",
    "        gamma = self.gammafc(x) #? gammaLayer(beta)\n",
    "        return beta,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block,self).__init__()\n",
    "        self.fc1 = nn.Linear(256,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        \n",
    "    def forward(self,y):\n",
    "        x = y['ex']\n",
    "        gamma = y['gamma']\n",
    "        beta = y['beta']\n",
    "        #First apply Conditional Batch Normalization\n",
    "        out = gamma*self.bn1(x) + beta\n",
    "        #Then ReLU activation function\n",
    "        out = F.relu(out)\n",
    "        #fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        #Second CBN layer\n",
    "        out = gamma*self.bn2(out) + beta\n",
    "        #RELU activation\n",
    "        out = F.relu(out)\n",
    "        #2nd fully connected\n",
    "        out = self.fc2(out)\n",
    "        #Add to the input of the ResNet Block \n",
    "        out = x + out\n",
    "        \n",
    "        return {'ex':out, 'beta':beta, 'gamma':gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OccupancyModel,self).__init__()\n",
    "        self.blocks = self.makeBlocks()\n",
    "        self.encoder = ImageEncoder()\n",
    "        self.cbn = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(3,256)\n",
    "        self.fc2 = nn.Linear(256,1)\n",
    "        \n",
    "    def makeBlocks(self):\n",
    "        blocks = []\n",
    "        for _ in range(5):\n",
    "            blocks.append(Block())\n",
    "        return nn.Sequential(*blocks)\n",
    "   \n",
    "  \n",
    "    def forward(self,x,img):\n",
    "        gamma,beta = self.encoder(img)\n",
    "        x = self.fc1(x)\n",
    "        #5 pre-activation ResNet-blocks\n",
    "        x = self.blocks({'gamma':gamma, 'beta':beta, 'ex':x })\n",
    "        x = x['ex']\n",
    "        x = gamma*self.cbn(x) + beta\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OccupancyModel()\n",
    "#Input to the occupancy network architecture is the \n",
    "#output of the encoder network and a batch of 3D coordinates. \n",
    "coords = torch.rand(64,3)\n",
    "image = torch.rand(64,3,7,7)\n",
    "model.eval()\n",
    "\n",
    "p = model(coords,image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load some data:\n",
    "#The .npz contains \"points, occupancies, loc, scale\" \n",
    "with numpy.load(\"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks/data/ShapeNet/02691156/fd528602cbde6f11bbf3143b1cb6076a/points.npz\") as data:\n",
    "    pts = torch.tensor(data[\"points\"], dtype=torch.float)\n",
    "    occupancies = torch.tensor(numpy.unpackbits(data[\"occupancies\"])[:pts.size()[0]], dtype=torch.float)\n",
    "\n",
    "image = numpy.array(Image.open(\"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks/data/ShapeNet/02691156/fd528602cbde6f11bbf3143b1cb6076a/img_choy2016/015.jpg\"))\n",
    "#At least for this image directory, the jpgs come in as 137,137,3\n",
    "image = torch.tensor(image,dtype=torch.float).permute(2,0,1)\n",
    "image = image.view(1,3,137,137)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(pts,occupancies)), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, trainloader, optimizer):\n",
    "    modelCriterion = nn.BCELoss()\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        (images, pts, occupancies) = data\n",
    "        #Each batch size contains batch_size sets of \"K\" points\n",
    "        #Collapse those two dimensions\n",
    "        images = images.view(batch_size*K,3,137,137).cuda() #make robust\n",
    "        pts = pts.view(batch_size*K, 3).cuda()\n",
    "        occupancies = occupancies.view(batch_size*K,1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(pts, images) #a probability for each point \n",
    "        loss = modelCriterion(output, occupancies)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a category and load all of the available data:\n",
    "#\"name:table, id: 04379243\"\n",
    "import random\n",
    "topdir = \"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks\"\n",
    "imageFiles = [\"000.jpg\",\"001.jpg\", \"002.jpg\",\"003.jpg\", \"004.jpg\", \"005.jpg\", \"006.jpg\", \"007.jpg\", \"008.jpg\",\n",
    "             \"009.jpg\", \"010.jpg\", \"011.jpg\", \"012.jpg\", \"013.jpg\", \"014.jpg\", \"015.jpg\", \"016.jpg\", \"017.jpg\",\n",
    "             \"018.jpg\", \"019.jpg\", \"020.jpg\", \"023.jpg\"]\n",
    "\n",
    "#One DataSetClass per subdirectory in a category, will return \"K\" point samples and a single image randomly\n",
    "#drawn from the 23 available\n",
    "class DataSetClass(torch.utils.data.Dataset):\n",
    "    def __init__(self, d):\n",
    "        self.dir = d\n",
    "        with numpy.load(f\"{d}/points.npz\") as data:\n",
    "            self.pts = torch.tensor(data[\"points\"], dtype=torch.float)\n",
    "            self.occupancies = torch.tensor(numpy.unpackbits(data[\"occupancies\"])[:self.pts.size()[0]], dtype=torch.float)\n",
    "        self.K = K #TODO how many sample points should come in? \n",
    "        self.length = int(100000/self.K)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        #pick an image randomly to be used an observation for this set of \"K sample points\"\n",
    "        imageFile = imageFiles[random.randint(0, len(imageFiles)-1)]\n",
    "        with Image.open(f\"{self.dir}/img_choy2016/{imageFile}\") as image:\n",
    "                image = numpy.array(image)\n",
    "                image = torch.tensor(image,dtype=torch.float)\n",
    "                #if the image is grey scale, stack 3 to conform dimensions\n",
    "                if len(image.size()) < 3:\n",
    "                    image = torch.stack([image, image, image])\n",
    "                else:\n",
    "                    image = image.permute(2,0,1)\n",
    "        #Stack identical copies of the image so we have one for each input point\n",
    "        #Maybe there is a better way\n",
    "        image = torch.stack([image for _ in range(self.K)])\n",
    "        #sampling in order is fine? \n",
    "        return image, self.pts[idx*self.K:(idx*self.K+self.K)], self.occupancies[idx*self.K:(idx*self.K+self.K)]\n",
    "\n",
    "       \n",
    "#catalogue all of the directories with the chosen category\n",
    "trainingDirs = []\n",
    "tablesDirectory=f\"{topdir}/data/ShapeNet/02828884\"\n",
    "with io.open(f\"{tablesDirectory}/test.lst\") as testlist:\n",
    "    for testdir in testlist.readlines():\n",
    "        trainingDirs.append(f\"{tablesDirectory}/{testdir.strip()}\")\n",
    "dataSets = []\n",
    "for tdir in trainingDirs:\n",
    "    dataSets.append(DataSetClass(tdir))\n",
    "data = torch.utils.data.ConcatDataset(dataSets)\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1452000 (0%)]\\tLoss: 0.804662\n",
      "Train Epoch: 1 [30/1452000 (0%)]\\tLoss: 0.086864\n",
      "Train Epoch: 1 [60/1452000 (0%)]\\tLoss: 0.031965\n",
      "Train Epoch: 1 [90/1452000 (0%)]\\tLoss: 0.085044\n",
      "Train Epoch: 1 [120/1452000 (0%)]\\tLoss: 0.069606\n",
      "Train Epoch: 1 [150/1452000 (0%)]\\tLoss: 0.102564\n",
      "Train Epoch: 1 [180/1452000 (0%)]\\tLoss: 0.055849\n",
      "Train Epoch: 1 [210/1452000 (0%)]\\tLoss: 0.086204\n",
      "Train Epoch: 1 [240/1452000 (0%)]\\tLoss: 0.081336\n",
      "Train Epoch: 1 [270/1452000 (0%)]\\tLoss: 0.073000\n",
      "Train Epoch: 1 [300/1452000 (0%)]\\tLoss: 0.104006\n",
      "Train Epoch: 1 [330/1452000 (0%)]\\tLoss: 0.104545\n",
      "Train Epoch: 1 [360/1452000 (0%)]\\tLoss: 0.079840\n",
      "Train Epoch: 1 [390/1452000 (0%)]\\tLoss: 0.085420\n",
      "Train Epoch: 1 [420/1452000 (0%)]\\tLoss: 0.084836\n"
     ]
    }
   ],
   "source": [
    "model = OccupancyModel().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "train(1,model,train_loader,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
