{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Encoder network for single view 3D reconstruction is a ResNet18 pretrained\n",
    "#on the ImageNet dataset with the last fully-connected layer adjusted to project\n",
    "#the features to a 256 dimensional embedding, \"c\"\n",
    "from torchvision.models.resnet import resnet18 as _resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder,self).__init__()\n",
    "        self.encoderModel = _resnet18(pretrained=True)\n",
    "        self.fc1 = nn.Linear(1000, 256)\n",
    "        self.betafc = nn.Linear(256,256)\n",
    "        self.gammafc = nn.Linear(256,256)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoderModel(x)\n",
    "        #project to 256 dimensional embedding \n",
    "        x = self.fc1(x)\n",
    "        # Obtain Beta and gamma inputs into conditional batch normalization\n",
    "        # QUESTION Are these split or one after the other?\n",
    "        beta = self.betafc(x)\n",
    "        gamma = self.gammafc(x) #? gammaLayer(beta)\n",
    "        return beta,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block,self).__init__()\n",
    "        self.fc1 = nn.Linear(256,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        \n",
    "    def forward(self,y):\n",
    "        x = y['ex']\n",
    "        gamma = y['gamma']\n",
    "        beta = y['beta']\n",
    "        #First apply Conditional Batch Normalization\n",
    "        out = gamma*self.bn1(x) + beta\n",
    "        #Then ReLU activation function\n",
    "        out = F.relu(out)\n",
    "        #fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        #Second CBN layer\n",
    "        out = gamma*self.bn2(out) + beta\n",
    "        #RELU activation\n",
    "        out = F.relu(out)\n",
    "        #2nd fully connected\n",
    "        out = self.fc2(out)\n",
    "        #Add to the input of the ResNet Block \n",
    "        out = x + out\n",
    "        \n",
    "        return {'ex':out, 'beta':beta, 'gamma':gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OccupancyModel,self).__init__()\n",
    "        self.blocks = self.makeBlocks()\n",
    "        self.encoder = ImageEncoder()\n",
    "        self.cbn = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(3,256)\n",
    "        self.fc2 = nn.Linear(256,1)\n",
    "        \n",
    "    def makeBlocks(self):\n",
    "        blocks = []\n",
    "        for _ in range(5):\n",
    "            blocks.append(Block())\n",
    "        return nn.Sequential(*blocks)\n",
    "   \n",
    "  \n",
    "    def forward(self,x, img):\n",
    "        gamma,beta = self.encoder(img)\n",
    "        x = self.fc1(x)\n",
    "        #5 pre-activation ResNet-blocks\n",
    "        x = self.blocks({'gamma':gamma, 'beta':beta, 'ex':x })\n",
    "        x = x['ex']\n",
    "        x = gamma*self.cbn(x) + beta\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OccupancyModel()\n",
    "#Input to the occupancy network architecture is the \n",
    "#output of the encoder network and a batch of 3D coordinates. \n",
    "coords = torch.rand(64,3)\n",
    "image = torch.rand(64,3,7,7)\n",
    "model.eval()\n",
    "\n",
    "p = model(coords,image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load some data:\n",
    "#The .npz contains \"points, occupancies, loc, scale\" \n",
    "import numpy\n",
    "with numpy.load(\"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks/data/ShapeNet/02691156/fd528602cbde6f11bbf3143b1cb6076a/points.npz\") as data:\n",
    "    pts = torch.tensor(data[\"points\"], dtype=torch.float)\n",
    "    occupancies = torch.tensor(numpy.unpackbits(data[\"occupancies\"])[:pts.size()[0]], dtype=torch.float)\n",
    "\n",
    "from PIL import Image\n",
    "image = numpy.array(Image.open(\"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks/data/ShapeNet/02691156/fd528602cbde6f11bbf3143b1cb6076a/img_choy2016/015.jpg\"))\n",
    "#At least for this image directory, the jpgs come in as 137,137,3\n",
    "image = torch.tensor(image,dtype=torch.float).permute(2,0,1)\n",
    "image = image.view(1,3,137,137)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(pts,occupancies)), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, trainloader, optimizer):\n",
    "    modelCriterion = nn.BCELoss()\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        (pts, occupancies) = data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(pts, image) #a probability for each point \n",
    "        loss = modelCriterion(output, occupancies)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/100000 (0%)]\\tLoss: 0.001071\n",
      "Train Epoch: 1 [20/100000 (1%)]\\tLoss: 0.000012\n",
      "Train Epoch: 1 [40/100000 (1%)]\\tLoss: 0.003240\n",
      "Train Epoch: 1 [60/100000 (2%)]\\tLoss: 0.009982\n",
      "Train Epoch: 1 [80/100000 (3%)]\\tLoss: 0.064105\n",
      "Train Epoch: 1 [100/100000 (3%)]\\tLoss: 0.040212\n",
      "Train Epoch: 1 [120/100000 (4%)]\\tLoss: 0.003784\n",
      "Train Epoch: 1 [140/100000 (4%)]\\tLoss: 0.092932\n",
      "Train Epoch: 1 [160/100000 (5%)]\\tLoss: 0.008418\n",
      "Train Epoch: 1 [180/100000 (6%)]\\tLoss: 0.008762\n",
      "Train Epoch: 1 [200/100000 (6%)]\\tLoss: 0.048621\n",
      "Train Epoch: 1 [220/100000 (7%)]\\tLoss: 0.004228\n",
      "Train Epoch: 1 [240/100000 (8%)]\\tLoss: 0.006790\n",
      "Train Epoch: 1 [260/100000 (8%)]\\tLoss: 0.004797\n",
      "Train Epoch: 1 [280/100000 (9%)]\\tLoss: 0.041292\n",
      "Train Epoch: 1 [300/100000 (10%)]\\tLoss: 0.067860\n",
      "Train Epoch: 1 [320/100000 (10%)]\\tLoss: 0.004231\n",
      "Train Epoch: 1 [340/100000 (11%)]\\tLoss: 0.002475\n",
      "Train Epoch: 1 [360/100000 (12%)]\\tLoss: 0.107249\n",
      "Train Epoch: 1 [380/100000 (12%)]\\tLoss: 0.012398\n",
      "Train Epoch: 1 [400/100000 (13%)]\\tLoss: 0.001745\n",
      "Train Epoch: 1 [420/100000 (13%)]\\tLoss: 0.008754\n",
      "Train Epoch: 1 [440/100000 (14%)]\\tLoss: 0.002487\n",
      "Train Epoch: 1 [460/100000 (15%)]\\tLoss: 0.009591\n",
      "Train Epoch: 1 [480/100000 (15%)]\\tLoss: 0.259473\n",
      "Train Epoch: 1 [500/100000 (16%)]\\tLoss: 0.055811\n",
      "Train Epoch: 1 [520/100000 (17%)]\\tLoss: 0.004108\n",
      "Train Epoch: 1 [540/100000 (17%)]\\tLoss: 0.001634\n",
      "Train Epoch: 1 [560/100000 (18%)]\\tLoss: 0.003418\n",
      "Train Epoch: 1 [580/100000 (19%)]\\tLoss: 0.007292\n",
      "Train Epoch: 1 [600/100000 (19%)]\\tLoss: 0.034900\n",
      "Train Epoch: 1 [620/100000 (20%)]\\tLoss: 0.012758\n",
      "Train Epoch: 1 [640/100000 (20%)]\\tLoss: 0.044486\n",
      "Train Epoch: 1 [660/100000 (21%)]\\tLoss: 0.077938\n",
      "Train Epoch: 1 [680/100000 (22%)]\\tLoss: 0.004699\n",
      "Train Epoch: 1 [700/100000 (22%)]\\tLoss: 0.012657\n",
      "Train Epoch: 1 [720/100000 (23%)]\\tLoss: 0.032651\n",
      "Train Epoch: 1 [740/100000 (24%)]\\tLoss: 0.006407\n",
      "Train Epoch: 1 [760/100000 (24%)]\\tLoss: 0.006875\n",
      "Train Epoch: 1 [780/100000 (25%)]\\tLoss: 0.001233\n",
      "Train Epoch: 1 [800/100000 (26%)]\\tLoss: 0.021830\n",
      "Train Epoch: 1 [820/100000 (26%)]\\tLoss: 0.002186\n",
      "Train Epoch: 1 [840/100000 (27%)]\\tLoss: 0.004379\n",
      "Train Epoch: 1 [860/100000 (28%)]\\tLoss: 0.001906\n",
      "Train Epoch: 1 [880/100000 (28%)]\\tLoss: 0.000785\n",
      "Train Epoch: 1 [900/100000 (29%)]\\tLoss: 0.003615\n",
      "Train Epoch: 1 [920/100000 (29%)]\\tLoss: 0.000574\n",
      "Train Epoch: 1 [940/100000 (30%)]\\tLoss: 0.103653\n",
      "Train Epoch: 1 [960/100000 (31%)]\\tLoss: 0.001334\n",
      "Train Epoch: 1 [980/100000 (31%)]\\tLoss: 0.008814\n",
      "Train Epoch: 1 [1000/100000 (32%)]\\tLoss: 0.001281\n",
      "Train Epoch: 1 [1020/100000 (33%)]\\tLoss: 0.005556\n",
      "Train Epoch: 1 [1040/100000 (33%)]\\tLoss: 0.011805\n",
      "Train Epoch: 1 [1060/100000 (34%)]\\tLoss: 0.002063\n",
      "Train Epoch: 1 [1080/100000 (35%)]\\tLoss: 0.049691\n",
      "Train Epoch: 1 [1100/100000 (35%)]\\tLoss: 0.004002\n",
      "Train Epoch: 1 [1120/100000 (36%)]\\tLoss: 0.001287\n",
      "Train Epoch: 1 [1140/100000 (36%)]\\tLoss: 0.029785\n",
      "Train Epoch: 1 [1160/100000 (37%)]\\tLoss: 0.070618\n",
      "Train Epoch: 1 [1180/100000 (38%)]\\tLoss: 0.004665\n",
      "Train Epoch: 1 [1200/100000 (38%)]\\tLoss: 0.077540\n",
      "Train Epoch: 1 [1220/100000 (39%)]\\tLoss: 0.011522\n",
      "Train Epoch: 1 [1240/100000 (40%)]\\tLoss: 0.057934\n",
      "Train Epoch: 1 [1260/100000 (40%)]\\tLoss: 0.009068\n",
      "Train Epoch: 1 [1280/100000 (41%)]\\tLoss: 0.004182\n",
      "Train Epoch: 1 [1300/100000 (42%)]\\tLoss: 0.067807\n",
      "Train Epoch: 1 [1320/100000 (42%)]\\tLoss: 0.008078\n",
      "Train Epoch: 1 [1340/100000 (43%)]\\tLoss: 0.015752\n",
      "Train Epoch: 1 [1360/100000 (44%)]\\tLoss: 0.005867\n",
      "Train Epoch: 1 [1380/100000 (44%)]\\tLoss: 0.002049\n",
      "Train Epoch: 1 [1400/100000 (45%)]\\tLoss: 0.006065\n",
      "Train Epoch: 1 [1420/100000 (45%)]\\tLoss: 0.041379\n",
      "Train Epoch: 1 [1440/100000 (46%)]\\tLoss: 0.007449\n",
      "Train Epoch: 1 [1460/100000 (47%)]\\tLoss: 0.139569\n",
      "Train Epoch: 1 [1480/100000 (47%)]\\tLoss: 0.049375\n",
      "Train Epoch: 1 [1500/100000 (48%)]\\tLoss: 0.052159\n",
      "Train Epoch: 1 [1520/100000 (49%)]\\tLoss: 0.006857\n",
      "Train Epoch: 1 [1540/100000 (49%)]\\tLoss: 0.004502\n",
      "Train Epoch: 1 [1560/100000 (50%)]\\tLoss: 0.002909\n",
      "Train Epoch: 1 [1580/100000 (51%)]\\tLoss: 0.007757\n",
      "Train Epoch: 1 [1600/100000 (51%)]\\tLoss: 0.008882\n",
      "Train Epoch: 1 [1620/100000 (52%)]\\tLoss: 0.003523\n",
      "Train Epoch: 1 [1640/100000 (52%)]\\tLoss: 0.000791\n",
      "Train Epoch: 1 [1660/100000 (53%)]\\tLoss: 0.055691\n",
      "Train Epoch: 1 [1680/100000 (54%)]\\tLoss: 0.006886\n",
      "Train Epoch: 1 [1700/100000 (54%)]\\tLoss: 0.001421\n",
      "Train Epoch: 1 [1720/100000 (55%)]\\tLoss: 0.041570\n",
      "Train Epoch: 1 [1740/100000 (56%)]\\tLoss: 0.004254\n",
      "Train Epoch: 1 [1760/100000 (56%)]\\tLoss: 0.044388\n",
      "Train Epoch: 1 [1780/100000 (57%)]\\tLoss: 0.104399\n",
      "Train Epoch: 1 [1800/100000 (58%)]\\tLoss: 0.003545\n",
      "Train Epoch: 1 [1820/100000 (58%)]\\tLoss: 0.003390\n",
      "Train Epoch: 1 [1840/100000 (59%)]\\tLoss: 0.072325\n",
      "Train Epoch: 1 [1860/100000 (60%)]\\tLoss: 0.061694\n",
      "Train Epoch: 1 [1880/100000 (60%)]\\tLoss: 0.003211\n",
      "Train Epoch: 1 [1900/100000 (61%)]\\tLoss: 0.012251\n",
      "Train Epoch: 1 [1920/100000 (61%)]\\tLoss: 0.024698\n",
      "Train Epoch: 1 [1940/100000 (62%)]\\tLoss: 0.097152\n",
      "Train Epoch: 1 [1960/100000 (63%)]\\tLoss: 0.004367\n",
      "Train Epoch: 1 [1980/100000 (63%)]\\tLoss: 0.027747\n",
      "Train Epoch: 1 [2000/100000 (64%)]\\tLoss: 0.085205\n",
      "Train Epoch: 1 [2020/100000 (65%)]\\tLoss: 0.007062\n",
      "Train Epoch: 1 [2040/100000 (65%)]\\tLoss: 0.011896\n",
      "Train Epoch: 1 [2060/100000 (66%)]\\tLoss: 0.155226\n",
      "Train Epoch: 1 [2080/100000 (67%)]\\tLoss: 0.003218\n",
      "Train Epoch: 1 [2100/100000 (67%)]\\tLoss: 0.004201\n",
      "Train Epoch: 1 [2120/100000 (68%)]\\tLoss: 0.029313\n",
      "Train Epoch: 1 [2140/100000 (68%)]\\tLoss: 0.005776\n",
      "Train Epoch: 1 [2160/100000 (69%)]\\tLoss: 0.051252\n",
      "Train Epoch: 1 [2180/100000 (70%)]\\tLoss: 0.000532\n",
      "Train Epoch: 1 [2200/100000 (70%)]\\tLoss: 0.055981\n",
      "Train Epoch: 1 [2220/100000 (71%)]\\tLoss: 0.002558\n",
      "Train Epoch: 1 [2240/100000 (72%)]\\tLoss: 0.040132\n",
      "Train Epoch: 1 [2260/100000 (72%)]\\tLoss: 0.001443\n",
      "Train Epoch: 1 [2280/100000 (73%)]\\tLoss: 0.000802\n",
      "Train Epoch: 1 [2300/100000 (74%)]\\tLoss: 0.000861\n",
      "Train Epoch: 1 [2320/100000 (74%)]\\tLoss: 0.000510\n",
      "Train Epoch: 1 [2340/100000 (75%)]\\tLoss: 0.034982\n",
      "Train Epoch: 1 [2360/100000 (75%)]\\tLoss: 0.029609\n",
      "Train Epoch: 1 [2380/100000 (76%)]\\tLoss: 0.004929\n",
      "Train Epoch: 1 [2400/100000 (77%)]\\tLoss: 0.055278\n",
      "Train Epoch: 1 [2420/100000 (77%)]\\tLoss: 0.073763\n",
      "Train Epoch: 1 [2440/100000 (78%)]\\tLoss: 0.001088\n",
      "Train Epoch: 1 [2460/100000 (79%)]\\tLoss: 0.005650\n",
      "Train Epoch: 1 [2480/100000 (79%)]\\tLoss: 0.036014\n",
      "Train Epoch: 1 [2500/100000 (80%)]\\tLoss: 0.003484\n",
      "Train Epoch: 1 [2520/100000 (81%)]\\tLoss: 0.162931\n",
      "Train Epoch: 1 [2540/100000 (81%)]\\tLoss: 0.005947\n",
      "Train Epoch: 1 [2560/100000 (82%)]\\tLoss: 0.004219\n",
      "Train Epoch: 1 [2580/100000 (83%)]\\tLoss: 0.004080\n",
      "Train Epoch: 1 [2600/100000 (83%)]\\tLoss: 0.003312\n",
      "Train Epoch: 1 [2620/100000 (84%)]\\tLoss: 0.002020\n",
      "Train Epoch: 1 [2640/100000 (84%)]\\tLoss: 0.058110\n",
      "Train Epoch: 1 [2660/100000 (85%)]\\tLoss: 0.050332\n",
      "Train Epoch: 1 [2680/100000 (86%)]\\tLoss: 0.005551\n",
      "Train Epoch: 1 [2700/100000 (86%)]\\tLoss: 0.044158\n",
      "Train Epoch: 1 [2720/100000 (87%)]\\tLoss: 0.067146\n",
      "Train Epoch: 1 [2740/100000 (88%)]\\tLoss: 0.003027\n",
      "Train Epoch: 1 [2760/100000 (88%)]\\tLoss: 0.002636\n",
      "Train Epoch: 1 [2780/100000 (89%)]\\tLoss: 0.006965\n",
      "Train Epoch: 1 [2800/100000 (90%)]\\tLoss: 0.003303\n",
      "Train Epoch: 1 [2820/100000 (90%)]\\tLoss: 0.006492\n",
      "Train Epoch: 1 [2840/100000 (91%)]\\tLoss: 0.036916\n",
      "Train Epoch: 1 [2860/100000 (91%)]\\tLoss: 0.004562\n",
      "Train Epoch: 1 [2880/100000 (92%)]\\tLoss: 0.002047\n",
      "Train Epoch: 1 [2900/100000 (93%)]\\tLoss: 0.047970\n",
      "Train Epoch: 1 [2920/100000 (93%)]\\tLoss: 0.008339\n",
      "Train Epoch: 1 [2940/100000 (94%)]\\tLoss: 0.060805\n",
      "Train Epoch: 1 [2960/100000 (95%)]\\tLoss: 0.025817\n",
      "Train Epoch: 1 [2980/100000 (95%)]\\tLoss: 0.052844\n",
      "Train Epoch: 1 [3000/100000 (96%)]\\tLoss: 0.004022\n",
      "Train Epoch: 1 [3020/100000 (97%)]\\tLoss: 0.038221\n",
      "Train Epoch: 1 [3040/100000 (97%)]\\tLoss: 0.008448\n",
      "Train Epoch: 1 [3060/100000 (98%)]\\tLoss: 0.000723\n",
      "Train Epoch: 1 [3080/100000 (99%)]\\tLoss: 0.006203\n",
      "Train Epoch: 1 [3100/100000 (99%)]\\tLoss: 0.001179\n",
      "Train Epoch: 1 [3120/100000 (100%)]\\tLoss: 0.000725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "train(1,model,train_loader,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
