{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 64\n",
    "batch_size = 64\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OM_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OM_Encoder,self).__init__()\n",
    "        self.fc1 = nn.Linear(3,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(256,128)\n",
    "        self.mean_fc = nn.Linear(128,128)\n",
    "        self.logstddev_fc = nn.Linear(128,128)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.squeeze()\n",
    "        n, c, k = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        pooled = F.max_pool1d(x, k).expand(x.size())\n",
    "        x = torch.cat([x,pooled],dim=1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        pooled = F.max_pool1d(x, k)\n",
    "        pooled = pooled.expand(x.size())\n",
    "        x = torch.cat([x,pooled],dim=1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = F.max_pool1d(x, k)\n",
    "        x= x.squeeze()\n",
    "\n",
    "        mean = self.mean_fc(x)\n",
    "        stddev = self.logstddev_fc(x)\n",
    "        \n",
    "        return mean,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block,self).__init__()\n",
    "        self.fc1 = nn.Conv2d(256,256,kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(256,256,kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.gammaLayer1 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.gammaLayer2 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer1 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer2 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        \n",
    "    def forward(self,y):\n",
    "        x = y['ex']\n",
    "        n,c,k,d = x.size()\n",
    "\n",
    "        encoding = y['enc']\n",
    "        gamma = self.gammaLayer1(encoding)\n",
    "\n",
    "        #Need to stack the beta and gamma\n",
    "        #so that we multiply all the points for one mesh\n",
    "        #by the same value\n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "        \n",
    "        beta = self.betaLayer1(encoding)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "\n",
    "        #First apply Conditional Batch Normalization\n",
    "        out = gamma*self.bn1(x) + beta\n",
    "        #Then ReLU activation function\n",
    "        out = F.relu(out)\n",
    "        #fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        #Second CBN layer\n",
    "        gamma = self.gammaLayer2(encoding)\n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "\n",
    "        beta = self.betaLayer2(encoding)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "        \n",
    "        out = gamma* self.bn2(out) + beta\n",
    "        #RELU activation\n",
    "        out = F.relu(out)\n",
    "        #2nd fully connected\n",
    "        out = self.fc2(out)\n",
    "        #Add to the input of the ResNet Block \n",
    "        out = x + out\n",
    "        \n",
    "        return {'ex':out, 'enc':encoding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OccupancyModel,self).__init__()\n",
    "        self.blocks = self.makeBlocks()\n",
    "        self.encoderModel = OM_Encoder()\n",
    "        self.gammaLayer = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.cbn = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.fc1 = nn.Conv2d(3,256,kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(256,1,kernel_size=1)\n",
    "        \n",
    "    def makeBlocks(self):\n",
    "        blocks = []\n",
    "        for _ in range(5):\n",
    "            blocks.append(Block())\n",
    "        return nn.Sequential(*blocks)\n",
    "   \n",
    "    def sampleFromZDist(self, z):\n",
    "        mean, logstddev = z\n",
    "        std = logstddev.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(logstddev,requires_grad=True)\n",
    "        return eps.mul(std).add_(mean)\n",
    "        \n",
    "    def forward(self,x, z_eval=None):\n",
    "        if self.training:\n",
    "            z_dist = self.encoderModel(x)\n",
    "            z = self.sampleFromZDist(z_dist)\n",
    "            z = z.unsqueeze(-1)\n",
    "        else:\n",
    "            z = z_eval\n",
    "            z_dist = (0,1)\n",
    "        x = self.fc1(x)\n",
    "        #5 pre-activation ResNet-blocks\n",
    "        x = self.blocks({'enc':z, 'ex':x })\n",
    "        x = x['ex']\n",
    "        n,c,k,d = x.size()\n",
    "        \n",
    "        #CBN\n",
    "        gamma = self.gammaLayer(z)\n",
    "        \n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "        \n",
    "        beta = self.betaLayer(z)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "\n",
    "        x = gamma.mul(self.cbn(x)).add_(beta)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = x.view(-1,1)\n",
    "        #x = torch.sigmoid(x)\n",
    "        return x, z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a category and load all of the available data:\n",
    "import random\n",
    "topdir = \"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks\"\n",
    "\n",
    "#One DataSetClass per subdirectory in a category, will return \"K\" point samples and a single image randomly\n",
    "#drawn from the 23 available\n",
    "class DataSetClass(torch.utils.data.Dataset):\n",
    "    def __init__(self, d):\n",
    "        self.dir = d\n",
    "        with numpy.load(f\"{d}/points.npz\") as data:\n",
    "            self.pts = torch.tensor(data[\"points\"], dtype=torch.float)\n",
    "            self.occupancies = torch.tensor(numpy.unpackbits(data[\"occupancies\"])[:self.pts.size()[0]], dtype=torch.float)\n",
    "        self.K = K \n",
    "        self.length = int(self.occupancies.size()[0]/self.K)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.pts[idx*self.K:(idx*self.K+self.K)], self.occupancies[idx*self.K:(idx*self.K+self.K)]\n",
    "\n",
    "       \n",
    "#catalogue all of the directories with the chosen category\n",
    "trainingDirs = []\n",
    "couchesDirectory=f\"{topdir}/data/ShapeNet/04256520\"\n",
    "\n",
    "#Get the test data\n",
    "testDirs = []\n",
    "with io.open(f\"{couchesDirectory}/test.lst\") as testlist:\n",
    "    for testdir in testlist.readlines():\n",
    "        testDirs.append(f\"{couchesDirectory}/{testdir.strip()}\")\n",
    "dataSets = []\n",
    "for tdir in testDirs:\n",
    "    dataSets.append(DataSetClass(tdir))\n",
    "test_data = torch.utils.data.ConcatDataset(dataSets)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGrid(ncuts, xl, xr, yl, yr, zl, zr):\n",
    "    #The unit cube centered at 0\n",
    "    #Subdivided into a grid of 32^3 \"voxels\"\n",
    "    x = numpy.linspace(xl,xr,ncuts)\n",
    "    y = numpy.linspace(yl,yr,ncuts)\n",
    "    z = numpy.linspace(zl,zr,ncuts)\n",
    "    xg,yg,zg = numpy.meshgrid(x,y,z)\n",
    "    x = torch.tensor(xg)\n",
    "    y = torch.tensor(yg)\n",
    "    z = torch.tensor(zg)\n",
    "    #Convert to a grid of 3 dimensional coordinate\n",
    "    tgrid = torch.stack([x,y,z], dim=3).permute(1,0,2,3)\n",
    "    #A cube is made up the 8 vertices\n",
    "    #Convert to a list where every 8 coords denote a cube\n",
    "    gridpts = torch.zeros(8*(ncuts-1)*(ncuts-1)*(ncuts-1),3)\n",
    "\n",
    "    '''\n",
    "    Vertex Order for marching cubes is \n",
    "    (0,0,0):(1,0,0):(1,1,0):(0,1,0):(0,0,1):(1,0,1):(1,1,1):(0,1,1)\n",
    "    '''\n",
    "    gpt = 0\n",
    "    for i in range(ncuts-1):\n",
    "        for j in range(ncuts-1):\n",
    "            for k in range(ncuts-1):\n",
    "                gridpts[gpt] = tgrid[i][j][k]\n",
    "                gridpts[gpt+1] = tgrid[i+1][j][k]\n",
    "                gridpts[gpt+2] = tgrid[i+1][j+1][k]\n",
    "                gridpts[gpt+3] = tgrid[i][j+1][k]\n",
    "                gridpts[gpt+4] = tgrid[i][j][k+1]\n",
    "                gridpts[gpt+5] = tgrid[i+1][j][k+1]\n",
    "                gridpts[gpt+6] = tgrid[i+1][j+1][k+1]\n",
    "                gridpts[gpt+7] = tgrid[i][j+1][k+1]\n",
    "                gpt = gpt + 8\n",
    "\n",
    "    return gridpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAdaptiveGrid(ncuts, xl, xh, yl, yh, zl, zh, limit,meshFunct, onCuda):\n",
    "    if not limit:\n",
    "        return None \n",
    "    g = generateGrid(ncuts, xl, xh, yl, yh, zl, zh)\n",
    "    ag = g.view(-1,3)\n",
    "    \n",
    "    finalGrid = []\n",
    "    #divide list of coordinates into cubes\n",
    "    for i in range(0,int(ag.size()[0]),8):\n",
    "        #marking one active if occupancies differ on the vertices \n",
    "        active = False\n",
    "        for k in range(0,8):\n",
    "            coord = ag[i + k]\n",
    "            if onCuda:\n",
    "                coord = coord.cuda()\n",
    "            active ^= meshFunct(coord)\n",
    "        if(active):\n",
    "            #near left coordinate is v0\n",
    "            nl = ag[i]\n",
    "            #top right coordinate is v6\n",
    "            tr = ag[i + 6]\n",
    "\n",
    "            #subdivide this cube into 8 subvoxels\n",
    "            g = generateAdaptiveGrid(3,nl[0],tr[0], nl[1],tr[1],nl[2],tr[2], limit-1, meshFunct, onCuda)\n",
    "            #replace this grid where the cube was \n",
    "            if g is not None:\n",
    "                finalGrid.append(g)\n",
    "            #or keep this cube\n",
    "            else:\n",
    "                finalGrid.append(ag[i: i + 8])\n",
    "        else:\n",
    "            finalGrid.append(ag[i:i+8])\n",
    "    for i in range(len(finalGrid)):\n",
    "        finalGrid[i] = finalGrid[i].view(-1,3)\n",
    "    finalGrid = torch.cat(finalGrid)\n",
    "\n",
    "    return finalGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def overModelThreshold(model, pt,z):\n",
    "    x = model(pt.view(1,3,1),z)\n",
    "    return (x > 0.5).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model on an adaptive Grid\n",
    "model = OccupancyModel()\n",
    "model.load_state_dict(torch.load(\"../training/unconditional_model3.pth\",map_location=device))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "f = partial(overModelThreshold,model)\n",
    "#g = generateAdaptiveGrid(32,-0.5,0.5,-0.5,0.5,-0.5,0.5,3, f, True)\n",
    "#numpy.savetxt('ag_32_3.txt', g.detach().numpy())\n",
    "pts = torch.tensor(numpy.loadtxt('bench_ag_32_3.txt'), dtype=torch.float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test boundaries of the latent space\n",
    "z_two = torch.ones(1,128).mul(2)\n",
    "z_two = z_two.unsqueeze(-1).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p in pts:\n",
    "        c = p.view(1,3,1).cuda()\n",
    "        c = c.unsqueeze(-1)\n",
    "        pred, z_dist = model(c,z_two)\n",
    "        occ.append(torch.sigmoid(pred).cpu())\n",
    "    numpy.savetxt(f'couch_interp/couch_ag_preds_32_two.txt', occ)\n",
    "\n",
    "z_zeros = torch.zeros(1,128)\n",
    "z_zeros = z_zeros.unsqueeze(-1).cuda()\n",
    "with torch.no_grad():\n",
    "    for p in pts:\n",
    "        c = p.view(1,3,1).cuda()\n",
    "        c = c.unsqueeze(-1)\n",
    "        pred, z_dist = model(c,z_zeros)\n",
    "        occ.append(torch.sigmoid(pred).cpu())\n",
    "    numpy.savetxt(f'couch_interp/couch_ag_preds_32_zero.txt', occ)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "z1 = Variable(torch.randn(1,128))\n",
    "z1 = 2*z1.unsqueeze(-1).cuda()\n",
    "z2 = Variable(torch.randn(1,128))\n",
    "z2 = 2*z2.unsqueeze(-1).cuda()\n",
    "alpha = torch.tensor(numpy.linspace(0,1,20)).cuda()\n",
    "for i in range(20):\n",
    "    z = alpha[i]*z1 + (1-alpha[i])*z2\n",
    "    occ = []\n",
    "    with torch.no_grad():\n",
    "        for p in pts:\n",
    "            c = p.view(1,3,1).cuda()\n",
    "            c = c.unsqueeze(-1)\n",
    "            pred, z_dist = model(c,z)\n",
    "            occ.append(torch.sigmoid(pred).cpu())\n",
    "    numpy.savetxt(f'couch_interp/couch_ag_preds_32_{i}.txt', occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.2038e-01],\n",
      "         [-2.5341e-01],\n",
      "         [ 7.9031e+00],\n",
      "         [-8.2451e-01],\n",
      "         [ 2.0335e+00],\n",
      "         [ 5.1307e-01],\n",
      "         [-6.7955e+00],\n",
      "         [ 1.7758e+00],\n",
      "         [-5.8149e+00],\n",
      "         [-1.0309e+00],\n",
      "         [-1.7001e+00],\n",
      "         [-1.0443e+00],\n",
      "         [-2.3428e-01],\n",
      "         [-1.4208e-01],\n",
      "         [ 2.8430e+00],\n",
      "         [ 1.5045e+00],\n",
      "         [-3.2517e+00],\n",
      "         [-4.2629e+00],\n",
      "         [ 2.8240e+00],\n",
      "         [ 1.4526e+00],\n",
      "         [-3.2044e-02],\n",
      "         [-2.0068e+00],\n",
      "         [-3.7496e-01],\n",
      "         [-4.5518e-01],\n",
      "         [-2.9235e+00],\n",
      "         [-2.6703e+00],\n",
      "         [ 6.1658e-01],\n",
      "         [-1.6193e-01],\n",
      "         [ 7.7285e+00],\n",
      "         [ 1.2071e+00],\n",
      "         [-4.0278e+00],\n",
      "         [-5.4895e-01],\n",
      "         [-1.0293e-01],\n",
      "         [-6.5336e+00],\n",
      "         [ 7.5186e+00],\n",
      "         [ 2.4896e+00],\n",
      "         [-2.2316e+00],\n",
      "         [ 4.9685e-01],\n",
      "         [-1.8322e+00],\n",
      "         [-1.4012e+00],\n",
      "         [-1.4534e+00],\n",
      "         [-8.0126e+00],\n",
      "         [ 1.2541e+00],\n",
      "         [-1.2522e+00],\n",
      "         [-4.8615e-01],\n",
      "         [-1.7245e-01],\n",
      "         [ 1.5168e+00],\n",
      "         [-3.7323e+00],\n",
      "         [-2.0475e+00],\n",
      "         [-2.2498e+00],\n",
      "         [ 6.9379e+00],\n",
      "         [ 4.2249e-02],\n",
      "         [ 7.6708e+00],\n",
      "         [-7.4913e+00],\n",
      "         [ 2.9268e+00],\n",
      "         [-3.4613e-01],\n",
      "         [-8.7499e-01],\n",
      "         [ 3.0945e+00],\n",
      "         [-7.2009e-01],\n",
      "         [-4.4558e+00],\n",
      "         [-9.3490e-01],\n",
      "         [ 1.6821e+00],\n",
      "         [-1.4791e+00],\n",
      "         [ 1.3552e+00],\n",
      "         [-7.9642e+00],\n",
      "         [ 4.7585e+00],\n",
      "         [-5.2097e+00],\n",
      "         [ 2.8100e+00],\n",
      "         [-7.2355e+00],\n",
      "         [-7.4269e-01],\n",
      "         [-3.0400e+00],\n",
      "         [ 8.3456e-01],\n",
      "         [ 5.3443e-01],\n",
      "         [ 7.0499e-03],\n",
      "         [ 1.0665e+00],\n",
      "         [ 5.3931e+00],\n",
      "         [-1.1970e+00],\n",
      "         [-5.1132e+00],\n",
      "         [ 5.6654e+00],\n",
      "         [ 2.9085e-01],\n",
      "         [-5.0415e+00],\n",
      "         [ 2.2052e+00],\n",
      "         [ 6.1929e+00],\n",
      "         [ 8.7547e-02],\n",
      "         [-7.4931e-01],\n",
      "         [ 6.4403e+00],\n",
      "         [ 3.5179e+00],\n",
      "         [-8.3273e+00],\n",
      "         [-5.3692e-01],\n",
      "         [-4.1241e+00],\n",
      "         [ 5.2115e+00],\n",
      "         [-3.8642e+00],\n",
      "         [ 6.8187e+00],\n",
      "         [-3.2673e-01],\n",
      "         [-1.9189e+00],\n",
      "         [ 6.8958e+00],\n",
      "         [ 2.0136e+00],\n",
      "         [-8.0283e-01],\n",
      "         [-4.8291e+00],\n",
      "         [ 1.3593e+00],\n",
      "         [-5.7022e+00],\n",
      "         [ 4.7252e+00],\n",
      "         [ 4.5480e-01],\n",
      "         [ 7.0603e+00],\n",
      "         [-4.0573e-01],\n",
      "         [ 1.0352e+01],\n",
      "         [ 2.3013e-02],\n",
      "         [-3.2222e+00],\n",
      "         [ 4.0621e+00],\n",
      "         [-2.5413e+00],\n",
      "         [-2.3204e+00],\n",
      "         [ 2.4772e+00],\n",
      "         [-3.0427e+00],\n",
      "         [ 2.4699e+00],\n",
      "         [ 3.4740e+00],\n",
      "         [-2.3251e+00],\n",
      "         [-3.5899e+00],\n",
      "         [ 1.9820e+00],\n",
      "         [ 5.4699e+00],\n",
      "         [ 3.9283e-01],\n",
      "         [-4.7934e-01],\n",
      "         [ 5.6476e+00],\n",
      "         [-2.1376e+00],\n",
      "         [-3.3825e+00],\n",
      "         [ 7.0308e-01],\n",
      "         [ 4.3454e+00],\n",
      "         [ 3.8959e+00],\n",
      "         [-1.7631e+00]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -8.1117],\n",
      "         [  2.2794],\n",
      "         [ -2.1641],\n",
      "         [  4.2820],\n",
      "         [ -1.7374],\n",
      "         [  3.3671],\n",
      "         [ -8.8443],\n",
      "         [  4.8213],\n",
      "         [  0.1848],\n",
      "         [ -1.3533],\n",
      "         [  3.3034],\n",
      "         [  3.6073],\n",
      "         [ -7.3811],\n",
      "         [  5.5755],\n",
      "         [  0.5596],\n",
      "         [  2.7734],\n",
      "         [ -2.8819],\n",
      "         [  1.2449],\n",
      "         [ -4.1304],\n",
      "         [  4.6924],\n",
      "         [ -4.0875],\n",
      "         [  1.2330],\n",
      "         [ -4.0459],\n",
      "         [  7.5213],\n",
      "         [  3.8411],\n",
      "         [  1.2168],\n",
      "         [  4.4229],\n",
      "         [ -0.9373],\n",
      "         [ -0.4339],\n",
      "         [  0.7123],\n",
      "         [  4.4891],\n",
      "         [  5.3945],\n",
      "         [  7.2350],\n",
      "         [ -2.9352],\n",
      "         [ -1.5161],\n",
      "         [ -0.5700],\n",
      "         [ -0.0453],\n",
      "         [  7.0088],\n",
      "         [ -6.8229],\n",
      "         [ -3.7562],\n",
      "         [  1.0994],\n",
      "         [  0.6102],\n",
      "         [  5.3797],\n",
      "         [  1.5265],\n",
      "         [  2.1118],\n",
      "         [  0.1531],\n",
      "         [ -2.2039],\n",
      "         [  8.0453],\n",
      "         [  0.3978],\n",
      "         [  0.6213],\n",
      "         [  0.0476],\n",
      "         [ -1.1379],\n",
      "         [  2.1576],\n",
      "         [-10.3809],\n",
      "         [ -2.6412],\n",
      "         [ -0.3659],\n",
      "         [  2.5305],\n",
      "         [ -0.3124],\n",
      "         [ -7.4918],\n",
      "         [  8.1611],\n",
      "         [  3.9732],\n",
      "         [  5.8780],\n",
      "         [ -4.5531],\n",
      "         [  9.1948],\n",
      "         [ -2.3022],\n",
      "         [  5.1773],\n",
      "         [ -6.9661],\n",
      "         [  8.6937],\n",
      "         [  3.8936],\n",
      "         [  2.8003],\n",
      "         [ -5.5370],\n",
      "         [ -1.5743],\n",
      "         [  2.4299],\n",
      "         [  2.2566],\n",
      "         [  8.6975],\n",
      "         [ -4.3091],\n",
      "         [  2.3851],\n",
      "         [  3.7578],\n",
      "         [  7.4787],\n",
      "         [  0.6551],\n",
      "         [  6.5398],\n",
      "         [  0.9828],\n",
      "         [ -3.4029],\n",
      "         [  3.0457],\n",
      "         [  2.4118],\n",
      "         [ -1.1673],\n",
      "         [  0.2571],\n",
      "         [ -1.5432],\n",
      "         [  0.1902],\n",
      "         [ -6.3904],\n",
      "         [  5.9261],\n",
      "         [  3.8185],\n",
      "         [  3.4310],\n",
      "         [  2.3671],\n",
      "         [ -1.1720],\n",
      "         [ -1.1324],\n",
      "         [ -0.3507],\n",
      "         [  4.1962],\n",
      "         [ 10.0648],\n",
      "         [ -3.6706],\n",
      "         [ -3.7789],\n",
      "         [ -1.3844],\n",
      "         [ -0.5032],\n",
      "         [  1.5148],\n",
      "         [  6.0276],\n",
      "         [  2.7861],\n",
      "         [ -2.5652],\n",
      "         [ -5.5703],\n",
      "         [  1.1486],\n",
      "         [ -1.3024],\n",
      "         [ -1.4279],\n",
      "         [  5.1811],\n",
      "         [ -0.7191],\n",
      "         [ -2.1710],\n",
      "         [  2.8638],\n",
      "         [  7.2663],\n",
      "         [  9.3100],\n",
      "         [ -9.3064],\n",
      "         [  4.0775],\n",
      "         [  1.2331],\n",
      "         [ -0.0755],\n",
      "         [  1.2421],\n",
      "         [ -0.8708],\n",
      "         [ -4.7743],\n",
      "         [  1.3659],\n",
      "         [ -4.5280],\n",
      "         [  2.1512],\n",
      "         [  1.7802]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
