{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 64\n",
    "batch_size = 64\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OM_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OM_Encoder,self).__init__()\n",
    "        self.fc1 = nn.Linear(3,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(256,128)\n",
    "        self.mean_fc = nn.Linear(128,128)\n",
    "        self.logstddev_fc = nn.Linear(128,128)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.squeeze()\n",
    "        n, c, k = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        pooled = F.max_pool1d(x, k).expand(x.size())\n",
    "        x = torch.cat([x,pooled],dim=1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        pooled = F.max_pool1d(x, k)\n",
    "        pooled = pooled.expand(x.size())\n",
    "        x = torch.cat([x,pooled],dim=1)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        n,k,c = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = F.max_pool1d(x, k)\n",
    "        x= x.squeeze()\n",
    "\n",
    "        mean = self.mean_fc(x)\n",
    "        stddev = self.logstddev_fc(x)\n",
    "        \n",
    "        return mean,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block,self).__init__()\n",
    "        self.fc1 = nn.Conv2d(256,256,kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(256,256,kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.bn2 = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.gammaLayer1 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.gammaLayer2 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer1 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer2 = nn.Conv1d(128,256,kernel_size=1)\n",
    "        \n",
    "    def forward(self,y):\n",
    "        x = y['ex']\n",
    "        n,c,k,d = x.size()\n",
    "\n",
    "        encoding = y['enc']\n",
    "        gamma = self.gammaLayer1(encoding)\n",
    "\n",
    "        #Need to stack the beta and gamma\n",
    "        #so that we multiply all the points for one mesh\n",
    "        #by the same value\n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "        \n",
    "        beta = self.betaLayer1(encoding)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "\n",
    "        #First apply Conditional Batch Normalization\n",
    "        out = gamma*self.bn1(x) + beta\n",
    "        #Then ReLU activation function\n",
    "        out = F.relu(out)\n",
    "        #fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        #Second CBN layer\n",
    "        gamma = self.gammaLayer2(encoding)\n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "\n",
    "        beta = self.betaLayer2(encoding)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "        \n",
    "        out = gamma* self.bn2(out) + beta\n",
    "        #RELU activation\n",
    "        out = F.relu(out)\n",
    "        #2nd fully connected\n",
    "        out = self.fc2(out)\n",
    "        #Add to the input of the ResNet Block \n",
    "        out = x + out\n",
    "        \n",
    "        return {'ex':out, 'enc':encoding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccupancyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OccupancyModel,self).__init__()\n",
    "        self.blocks = self.makeBlocks()\n",
    "        self.encoderModel = OM_Encoder()\n",
    "        self.gammaLayer = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.betaLayer = nn.Conv1d(128,256,kernel_size=1)\n",
    "        self.cbn = nn.BatchNorm2d(256, affine=False, track_running_stats=True)\n",
    "        self.fc1 = nn.Conv2d(3,256,kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(256,1,kernel_size=1)\n",
    "        \n",
    "    def makeBlocks(self):\n",
    "        blocks = []\n",
    "        for _ in range(5):\n",
    "            blocks.append(Block())\n",
    "        return nn.Sequential(*blocks)\n",
    "   \n",
    "    def sampleFromZDist(self, z):\n",
    "        mean, logstddev = z\n",
    "        std = logstddev.mul(0.5).exp_()\n",
    "        eps = torch.randn_like(logstddev,requires_grad=True)\n",
    "        return eps.mul(std).add_(mean)\n",
    "        \n",
    "    def forward(self,x, z_eval=None):\n",
    "        if self.training:\n",
    "            z_dist = self.encoderModel(x)\n",
    "            z = self.sampleFromZDist(z_dist)\n",
    "            z = z.unsqueeze(-1)\n",
    "        else:\n",
    "            z = z_eval\n",
    "            z_dist = (0,1)\n",
    "        x = self.fc1(x)\n",
    "        #5 pre-activation ResNet-blocks\n",
    "        x = self.blocks({'enc':z, 'ex':x })\n",
    "        x = x['ex']\n",
    "        n,c,k,d = x.size()\n",
    "        \n",
    "        #CBN\n",
    "        gamma = self.gammaLayer(z)\n",
    "        \n",
    "        gamma = torch.stack([gamma for _ in range(k)],dim=2)\n",
    "        \n",
    "        beta = self.betaLayer(z)\n",
    "        beta = torch.stack([beta for _ in range(k)],dim=2)\n",
    "\n",
    "        x = gamma.mul(self.cbn(x)).add_(beta)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = x.view(-1,1)\n",
    "        #x = torch.sigmoid(x)\n",
    "        return x, z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a category and load all of the available data:\n",
    "import random\n",
    "topdir = \"/home/andrea/Documents/GradSchool/OccupancyNetworks/occupancy_networks\"\n",
    "\n",
    "#One DataSetClass per subdirectory in a category, will return \"K\" point samples and a single image randomly\n",
    "#drawn from the 23 available\n",
    "class DataSetClass(torch.utils.data.Dataset):\n",
    "    def __init__(self, d):\n",
    "        self.dir = d\n",
    "        with numpy.load(f\"{d}/points.npz\") as data:\n",
    "            self.pts = torch.tensor(data[\"points\"], dtype=torch.float)\n",
    "            self.occupancies = torch.tensor(numpy.unpackbits(data[\"occupancies\"])[:self.pts.size()[0]], dtype=torch.float)\n",
    "        self.K = K \n",
    "        self.length = int(self.occupancies.size()[0]/self.K)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.pts[idx*self.K:(idx*self.K+self.K)], self.occupancies[idx*self.K:(idx*self.K+self.K)]\n",
    "\n",
    "       \n",
    "#catalogue all of the directories with the chosen category\n",
    "trainingDirs = []\n",
    "couchesDirectory=f\"{topdir}/data/ShapeNet/04256520\"\n",
    "\n",
    "#Get the test data\n",
    "testDirs = []\n",
    "with io.open(f\"{couchesDirectory}/test.lst\") as testlist:\n",
    "    for testdir in testlist.readlines():\n",
    "        testDirs.append(f\"{couchesDirectory}/{testdir.strip()}\")\n",
    "dataSets = []\n",
    "for tdir in testDirs:\n",
    "    dataSets.append(DataSetClass(tdir))\n",
    "test_data = torch.utils.data.ConcatDataset(dataSets)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGrid(ncuts, xl, xr, yl, yr, zl, zr):\n",
    "    #The unit cube centered at 0\n",
    "    #Subdivided into a grid of 32^3 \"voxels\"\n",
    "    x = numpy.linspace(xl,xr,ncuts)\n",
    "    y = numpy.linspace(yl,yr,ncuts)\n",
    "    z = numpy.linspace(zl,zr,ncuts)\n",
    "    xg,yg,zg = numpy.meshgrid(x,y,z)\n",
    "    x = torch.tensor(xg)\n",
    "    y = torch.tensor(yg)\n",
    "    z = torch.tensor(zg)\n",
    "    #Convert to a grid of 3 dimensional coordinate\n",
    "    tgrid = torch.stack([x,y,z], dim=3).permute(1,0,2,3)\n",
    "    #A cube is made up the 8 vertices\n",
    "    #Convert to a list where every 8 coords denote a cube\n",
    "    gridpts = torch.zeros(8*(ncuts-1)*(ncuts-1)*(ncuts-1),3)\n",
    "\n",
    "    '''\n",
    "    Vertex Order for marching cubes is \n",
    "    (0,0,0):(1,0,0):(1,1,0):(0,1,0):(0,0,1):(1,0,1):(1,1,1):(0,1,1)\n",
    "    '''\n",
    "    gpt = 0\n",
    "    for i in range(ncuts-1):\n",
    "        for j in range(ncuts-1):\n",
    "            for k in range(ncuts-1):\n",
    "                gridpts[gpt] = tgrid[i][j][k]\n",
    "                gridpts[gpt+1] = tgrid[i+1][j][k]\n",
    "                gridpts[gpt+2] = tgrid[i+1][j+1][k]\n",
    "                gridpts[gpt+3] = tgrid[i][j+1][k]\n",
    "                gridpts[gpt+4] = tgrid[i][j][k+1]\n",
    "                gridpts[gpt+5] = tgrid[i+1][j][k+1]\n",
    "                gridpts[gpt+6] = tgrid[i+1][j+1][k+1]\n",
    "                gridpts[gpt+7] = tgrid[i][j+1][k+1]\n",
    "                gpt = gpt + 8\n",
    "\n",
    "    return gridpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAdaptiveGrid(ncuts, xl, xh, yl, yh, zl, zh, limit,meshFunct, onCuda):\n",
    "    if not limit:\n",
    "        return None \n",
    "    g = generateGrid(ncuts, xl, xh, yl, yh, zl, zh)\n",
    "    ag = g.view(-1,3)\n",
    "    \n",
    "    finalGrid = []\n",
    "    #divide list of coordinates into cubes\n",
    "    for i in range(0,int(ag.size()[0]),8):\n",
    "        #marking one active if occupancies differ on the vertices \n",
    "        active = False\n",
    "        for k in range(0,8):\n",
    "            coord = ag[i + k]\n",
    "            if onCuda:\n",
    "                coord = coord.cuda()\n",
    "            active ^= meshFunct(coord)\n",
    "        if(active):\n",
    "            #near left coordinate is v0\n",
    "            nl = ag[i]\n",
    "            #top right coordinate is v6\n",
    "            tr = ag[i + 6]\n",
    "\n",
    "            #subdivide this cube into 8 subvoxels\n",
    "            g = generateAdaptiveGrid(3,nl[0],tr[0], nl[1],tr[1],nl[2],tr[2], limit-1, meshFunct, onCuda)\n",
    "            #replace this grid where the cube was \n",
    "            if g is not None:\n",
    "                finalGrid.append(g)\n",
    "            #or keep this cube\n",
    "            else:\n",
    "                finalGrid.append(ag[i: i + 8])\n",
    "        else:\n",
    "            finalGrid.append(ag[i:i+8])\n",
    "    for i in range(len(finalGrid)):\n",
    "        finalGrid[i] = finalGrid[i].view(-1,3)\n",
    "    finalGrid = torch.cat(finalGrid)\n",
    "\n",
    "    return finalGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def overModelThreshold(model, pt,z):\n",
    "    x = model(pt.view(1,3,1),z)\n",
    "    return (x > 0.5).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model on an adaptive Grid\n",
    "model = OccupancyModel()\n",
    "model.load_state_dict(torch.load(\"../training/unconditional_model3.pth\",map_location=device))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "f = partial(overModelThreshold,model)\n",
    "#g = generateAdaptiveGrid(32,-0.5,0.5,-0.5,0.5,-0.5,0.5,3, f, True)\n",
    "#numpy.savetxt('ag_32_3.txt', g.detach().numpy())\n",
    "pts = torch.tensor(numpy.loadtxt('bench_ag_32_3.txt'), dtype=torch.float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "z = Variable(torch.randn(1,128))\n",
    "z= z.unsqueeze(-1).cuda()\n",
    "for i in range(10):\n",
    "    #how to interpolate? add .1 to each dimension? \n",
    "    z.add_(0.1)\n",
    "    occ = []\n",
    "    with torch.no_grad():\n",
    "        for p in pts:\n",
    "            c = p.view(1,3,1).cuda()\n",
    "            c = c.unsqueeze(-1)\n",
    "            pred, z_dist = model(c,z)\n",
    "            occ.append(torch.sigmoid(pred).cpu())\n",
    "    numpy.savetxt(f'couch_interp/couch_ag_preds_32_{i}.txt', occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
